{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca43985",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd13758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hw_1_optimization import plot_levels, plot_trajectory\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229aec8",
   "metadata": {},
   "source": [
    "- Домашнее задание выполняется в этом же Jupyter Notebook'e. Можно использовать Google Colab, но прислать нужно выгруженный `.ipynb` ноутбук.\n",
    "\n",
    "- В названии файла укажите свои фамилию и имя\n",
    "\n",
    "- Решение каждой задачи/пункта задачи поместите после условия.\n",
    "\n",
    "- В финальной версии, которая будет отправлена на проверку, должны быть удалены все отладочные артефакты. Под таким артефактами подразумеваются любые выводы ячеек, которые никак не прокоментированы в тексте, а также любой массовый/длинный технический вывод.\n",
    "\n",
    "- При чистом перезапуске ноутбука (*Run -> Restart Kernel and Run All Cells* или *Kernel -> Restart & Run All* в юпитере) все ячейки должны выполняться без ошибок.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412691f",
   "metadata": {},
   "source": [
    "## Пример запуска метода.\n",
    "\n",
    "### Градиентный спуск в невыпуклом случае.\n",
    "\n",
    "Будем минимизировать функцию Розенброка - это хорошо известная (невыпуклая) функция для сравнения алгоритмов оптимизации\n",
    "\n",
    "Двумерная функция Розенброка выглядит так\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/Rosenbrock_function.svg/1920px-Rosenbrock_function.svg.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Минимум функции Розенброка находится в $x^* = (1, \\ldots, 1)^\\top$, значение функции в минимуме равно нулю.\n",
    "\n",
    "Реализуем градиентный спуск с постоянным шагом. Шаг грубо (с точостью до порядка) подберём руками - перебором найдём наибольшую степень десятки, при которой метод ещё сходится (не улетает на бесконечность).\n",
    "\n",
    "Чтобы нарисовать графики сходимости, на каждой итерации будем сохранять норму градиента, значение функции, расстояние до решения и текущую итерацию (на практике мы решения не знаем, но на тестовой задаче можем вычислить и эту метрику)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen, rosen_der\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def gd(f: Callable, df: Callable, x0: np.ndarray, x_star: np.ndarray, h: float, iters: int) -> tuple:\n",
    "    x = x0.copy()\n",
    "    grad_log, func_log, dist_log = [np.empty(iters) for _ in range(3)]\n",
    "    iter_log = np.empty((iters, x0.shape[0]))\n",
    "    for i in range(iters):\n",
    "        grad = df(x)\n",
    "        grad_log[i], func_log[i], dist_log[i], iter_log[i] = (\n",
    "            np.linalg.norm(grad),\n",
    "            f(x),\n",
    "            np.linalg.norm(x - x_star),\n",
    "            x.copy(),\n",
    "        )\n",
    "        x -= h * grad\n",
    "    return x, grad_log, func_log, dist_log, iter_log\n",
    "\n",
    "\n",
    "def nesterov(\n",
    "    f: Callable,\n",
    "    df: Callable,\n",
    "    x0: np.ndarray,\n",
    "    x_star: np.ndarray,\n",
    "    h: float,\n",
    "    momentum: float,\n",
    "    iters: int,\n",
    ") -> tuple:\n",
    "    x = x0.copy()\n",
    "    y = x0.copy()\n",
    "    grad_log, func_log, dist_log = [np.empty(iters) for _ in range(3)]\n",
    "    iter_log = np.empty((iters, x0.shape[0]))\n",
    "    for i in range(iters):\n",
    "        grad = df(y)\n",
    "        grad_log[i], func_log[i], dist_log[i], iter_log[i] = (\n",
    "            np.linalg.norm(grad),\n",
    "            f(x),\n",
    "            np.linalg.norm(x - x_star),\n",
    "            x.copy(),\n",
    "        )\n",
    "\n",
    "        x_old = x.copy()\n",
    "        x = y - h * grad\n",
    "        y = x + momentum * (x - x_old)\n",
    "\n",
    "    return x, grad_log, func_log, dist_log, iter_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "rosen_solution = np.ones(dim)\n",
    "\n",
    "x0 = np.zeros(dim)  # начальная точка\n",
    "\n",
    "_, _, _, _, iter_log_gd = gd(f=rosen, df=rosen_der, x0=x0, x_star=rosen_solution, h=2 * 1e-3, iters=1000)\n",
    "_, _, _, _, iter_log_nesterov = nesterov(\n",
    "    f=rosen, df=rosen_der, x0=x0, x_star=rosen_solution, h=2 * 1e-3, momentum=0.9, iters=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_levels(func=rosen, xrange=[-1.5, 1.5], yrange=[-1, 1.5])\n",
    "plot_trajectory(history=iter_log_nesterov, color=\"C2\", label=\"nesterov\")\n",
    "plot_trajectory(history=iter_log_gd, label=\"gd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137ea7e",
   "metadata": {},
   "source": [
    "## 1. Траектории градиентных методов на двумерной квадратичной функции.\n",
    "\n",
    "Рассмотрим траекторию градиентного метода на двумерной квадратичной функции вида\n",
    "$$\n",
    "f(x) = \\frac12 x^\\top A x,\n",
    "$$\n",
    "где $A\\in\\mathbb{S}_{++}^n$ -- симметричнач положительно определенная матрица $2\\times 2$. Проверьте, что константа гладкости $f$ равна $L = \\lambda_{\\max}(A)$, а константа сильной выпуклости равна $\\mu = \\lambda_{\\min}(A)$.\n",
    "\n",
    "Функция ```generate_random_2d_psd_matrix``` создает случайную матрицу с заданными собственными числами. Функция ```armijo``` вычисляет размер шага по правилу Армихо.\n",
    "\n",
    "**Задание**.\n",
    "\n",
    "1. Задайте квадратичную функцию и ее градиент.\n",
    "\n",
    "1. Меняя число обусловленности матрицы, запускайте из одной и той же точки градиентный метод с постоянным шагом, с выбором шага по Армихо и метод Нестерова. Измерьте количество итераций до достижения заданной точности (по аргументу). Также изобразите траектории с помощью ```plot_trajectory```. Сравните и объясните результаты. Размер шага задавайте методом проб и ошибок.\n",
    "\n",
    "**Замечание**. Здесь придется переписать функцию ```gd``` из введения так, чтобы можно было задавать шаг по условию Армихо. Также, для ускорения линейного поиска, можно передавать функции ```armijo``` размер шага, вычисленный на предыдущей итерации.\n",
    "\n",
    "3. Задайте размер шага для градиентного спуска и метода Нестерова согласно теории (шаг $1/L$). Происходят ли теперь осцилляции траектории метода? Объясните результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_1_optimization import generate_random_2d_psd_matrix, armijo\n",
    "\n",
    "# реализация и запуск методов, построение графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931288e3",
   "metadata": {},
   "source": [
    "## 2. Эффективность градиентных методов в зависимости от числа обусловленности.\n",
    "\n",
    "Из теории известно, что градиентному спуску нужно $O(\\kappa \\ln(1/\\varepsilon))$ итерация для достижения точности $\\varepsilon$, а методу Нестерова - $O(\\sqrt\\kappa \\ln(1/\\varepsilon))$, где $\\kappa = L/\\mu$ - число обусловленности функции. Задача это пункта - измерить данные зависимости на примере. Воспользуйтесь функцией ```generate_random_psd_matrix``` для генерации матрицы с нужным числом обусловленности.\n",
    "\n",
    "**Задание**.\n",
    "\n",
    "1. Запускайте градиентный спуск и метод Нестерова с теоретически заданными параметрами на задачах с разным числом обусловленности. Постройте график зависимости числа итераций до достижения нужной точности от числа обусловленности в двойном логарифмическом масштабе. Согласуется ли график с теорией?\n",
    "\n",
    "**Замечание**. Не сохраняйте историю итераций метода, т.к. это займет слишком много памяти. Измените соответствующее место в функции ```gd```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw_1_optimization import generate_random_psd_matrix\n",
    "\n",
    "# запуск методов и построение графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b94223",
   "metadata": {},
   "source": [
    "## 3. Логистическая регрессия."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be84c1",
   "metadata": {},
   "source": [
    "Задача [логистической регрессии](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%BE%D0%B3%D0%B8%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F) c $l_2$ регуляризацией\n",
    "для бинарной классификации имеет вид\n",
    "\n",
    "$\n",
    "f(x) = \\frac\\alpha2 \\|x\\|_2^2 + \\frac1m \\sum_{i=1}^m \\log (1 + \\exp(- b_i \\langle a_i, x \\rangle)) \\to \\min_w\n",
    "$\n",
    "\n",
    "где $x$-вектор параметров модели, $a_i$ - вектор признаков $i$-го объекта, $b_i \\in \\{-1, 1 \\}$ - метка класса $i$-го объекта. \n",
    "\n",
    "Скачиваем датасет (если команда выдаёт ошибку, можно руками скачать и положить файл `mushrooms.txt` в папку с ноутбуком)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/mushrooms\n",
    "!wget \"https://raw.githubusercontent.com/niquepolice/misc-files/refs/heads/master/mushrooms.txt\" -O mushrooms.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33fdd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_svmlight_file(\"mushrooms.txt\")\n",
    "X, y = data[0].toarray(), data[1]\n",
    "y = 2 * y - 3  # map {1, 2} to {-1, 1}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6aa9bf",
   "metadata": {},
   "source": [
    "**Задание**. \n",
    "\n",
    "1. Выпишите градиент и гессиан для данной задачи. Является ли задача выпуклой? А $\\mu$ - сильно выпуклой? Покажите, как можно оценить $\\mu$. Покажите, что константа гладкости может быть оценена как $L = \\max_i (\\|a_i\\|_2^2 / 4)$, взяв гессиан функции и использовав факт, что $e^t/(1 + e^t)^2\\leq 1/4$.\n",
    "\n",
    "1. Пользуясь формулой для $L$, численно оцените константу Липшица градиента для обучающей части выборки `X_{train}`, `y_{train}`.\n",
    "\n",
    "1. Реализуйте функцию логистической регрессии и ее градиент.\n",
    "\n",
    "1. Задайте $\\alpha \\approx L / 1000$. Запустите метод градиентного спуска для минимизации лосса на обучающей выборке. Длину шага определите из теории, подставив туда полученное $L$. Постройте графики сходимости по норме градиента от номера итерации (стоит сделать порядка 5000 итераций). Используйте логарифмический масштаб по вертикальной оси, чтобы можно было оценить скорость сходимости метода в окрестности решения. Посчитайте accuracy на тестовой выборке.\n",
    "\n",
    "1. Сравните градиентный метод с методом Нестерова с параметрами по теории и с методом с выбором шага по Армихо.\n",
    "\n",
    "1. Сделайте то же самое для $\\alpha = 0$. Что изменилось в графике сходимости?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# реализация и запуск методов, построение графиков"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
